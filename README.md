# Data Science Toolbox 4

By Sam, Kish and Dan

## Equity Split
  - 110% Dan Jones

## Setup

### Python environment
  1. Make new virtualenv: `python3 -m venv ./env`.
  2. Activate it: `source ./env/bin/activate/`.
  3. Install packages: `python3 -m pip install -r requirements.txt`.
  4. Launch Jupyter _from within the virtualenv_ and open `./project/report.ipynb`.


### Generating the dataset

**Note:** To avoid regenerating the whole dataset, and making life hard for Github's servers, download the full dataset from https://daniel.wilshirejones.com/private-uUX6IzfsRYLNiti4ZFmgv6U3dFInnq37r5YSQs46iejeB96q0MAy9Ko7hkgo/full-dataset.csv.gz and place it in `./data/full-dataset.csv.gz`.

To generate the dataset yourself, run the script `./project/dataset.py` from the command line. Use `-h` for help on it's usage:

```
$ python3 project/dataset.py --help
usage: dataset.py [-h] [--task TASK] [--dataset DATASET] [--repos REPOS]

Use the Github API to find, download and collate source code based on topics.
Split into two stages: 1) gather_repos :: generate a list of repos from the
Github API. 2) construct_dataset :: download and collate the repo list from 1)
into a single csv file where each line has the format: repo_id, language,
topics, file_contents.

optional arguments:
  -h, --help         show this help message and exit
  --task TASK        Task to complete: gather_repos or construct_dataset
  --dataset DATASET  Location to save the dataset. Used as the output of the
                     construct_dataset task.
  --repos REPOS      File location of the list of repos. This is generated by
                     the gather_repos task, and used as an input by the
                     construct_dataset task.
```
