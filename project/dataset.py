import argparse
import csv
import io
import json
import requests
import time
import zipfile


BASE_URL = "https://api.github.com"
SEARCH_URL = BASE_URL + "/search/repositories"

TOPICS = [
    "javascript",
    "python",
    "r",
    "shellcode",
    "payload",
]


def get_next_url(response_headers):
    """
    Little helper to handle pagination in Github API.
    """
    link_string = response_headers.get('Link')
    links = link_string.split(",")

    next_links = [link for link in links if 'rel="next"' in link]
    if next_links:
        [next_link] = next_links
        next_url = next_link.split(";")[0].strip(" <> ")

        return next_url

    return False


def gather_repo_list(topics):
    # Find the most popular repo's for each of the above topics
    repos = []
    
    for topic in topics:
        print(SEARCH_URL)
        response = requests.get(
            SEARCH_URL,
            params={
                "q": "topic:" + topic,
                "sort": "stars",
            },
            headers=headers,
        )
        time.sleep(6)
        repos += response.json().get("items")
    
        # Fetch all the searh results, not just the first page
        next_url = get_next_url(response.headers)
        while(next_url):
            print(next_url)
            response = requests.get(
                next_url,
                headers=headers,
            )
            time.sleep(6)
    
            repos += response.json().get("items")
    
            next_url = get_next_url(response.headers)

    return repos


def construct_data_set(repo_list, dataset_file):
    dataset_writer = csv.writer(
        dataset_file,
        delimiter=',',
        quoting=csv.QUOTE_ALL,
    )

    # We've picked topics that roughly map to programming languages, but
    # there will still be a mix in each repo. For example, Python projects
    # may include make files, or small bits of frontend javascript. Here,
    # we navigate through each repo and pull out files which match a
    # programming language we want to model, ignoring files that we don't
    # understand or care about (e.g. markdown).
    
    for repo in repo_list:
        print("Fetching " + repo['html_url'])

        # To download the repo as a whole, use the html url the api gives:
        response = requests.get(repo['html_url'] + '/archive/master.zip')

        time.sleep(10)  # be nice
    
        try:
            project_zip = zipfile.ZipFile(io.BytesIO(response.content))
    
            filenames = project_zip.namelist()
            for filename in filenames:
                if filename.lower().endswith('.js'):
                    language = 'javascript'
                elif filename.lower().endswith('.py'):
                    language = 'python'
                elif filename.lower().endswith('.r'):
                    language = 'r'
                else:
                    continue
        
                contents = project_zip.read(filename)
                dataset_writer.writerow([
                    repo['id'],
                    language,
                    ';'.join(repo.get('topics', [])),
                    contents,
                ])

        except Exception as exc:
            print(exc)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=(
       'Use the Github API to find, download and collate source '
       'code based on topics.\n'
       '\n'
       'Split into two stages: \n'
       '  1) gather_repos :: generate a list of repos from the Github API.\n'
       '  2) construct_dataset :: download and collate the repo list from 1) into a single csv file where each line has the format: repo_id, language, topics, file_contents.'
    ))

    parser.add_argument(
        '--task',
        dest='task',
        help="Task to complete: gather_repos or construct_dataset",
    )
    parser.add_argument(
        '--dataset',
        dest='dataset',
        help='Location to save the dataset. Used as the output of the construct_dataset task.',
    )
    parser.add_argument(
        '--repos',
        dest='repos',
        help='File location of the list of repos. This is generated by the gather_repos task, and used as an input by the construct_dataset task.',
    )
    
    args = parser.parse_args()


    if args.task == 'gather_repos' and args.repos:
        # GitHub throttles unauthenticated searches to 10 per minute, so just
        # wait 6 seconds between each request.
        # TODO: add authentication to requests so this is faster
        
        repos = gather_repo_list(TOPICS)
        
        # Save this repo list for later
        with open(args.repos, 'w') as repo_list_file:
            json.dump(repos, repo_list_file)


    if args.task == 'construct_dataset' and args.repos and args.dataset:

        # Load repo list back in
        with open(args.repos, 'r') as repo_list_file:
            repos = json.load(repo_list_file)
    
        javascript_repos = [
            repo for repo in repos
            if 'javascript' in repo['topics']
            and repo['language']
            and repo['language'].lower() == 'javascript'
        ]

        python_repos = [
            repo for repo in repos
            if 'python' in repo['topics']
            and repo['language']
            and repo['language'].lower() == 'python'
        ]

        r_repos = [
            repo for repo in repos
            if 'r' in repo['topics']
            and repo['language']
            and repo['language'].lower() == 'r'
        ]

        shellcode_repos = [
            repo for repo in repos
            if 'shellcode' in repo['topics']
            or 'payload' in repo['topics']
        ]
        
        # dataset file
        dataset_file = open(args.dataset, 'w')
        
        construct_data_set(
            javascript_repos[0:250] + python_repos[0:250] + r_repos[0:250] + shellcode_repos[0:250],
            dataset_file,
        )
        
        dataset_file.close()
