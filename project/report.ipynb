{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on Program Source Code\n",
    "---\n",
    "By Kishalay Banerjee, Dan Jones and Sam Harding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 0y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pickle\n",
    "import math\n",
    "import numpy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(0xC0FFEE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "There are a number of files which are too large to store on GitHub. These are hosted on our server, and can be downloaded by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'full_lda_model.pickle',\n",
    "    'full_tf.pickle',\n",
    "    'full_tf_vectorizer.pickle',\n",
    "    'full-dataset.csv.gz',\n",
    "]\n",
    "\n",
    "base_url = 'https://daniel.wilshirejones.com/private-uUX6IzfsRYLNiti4ZFmgv6U3dFInnq37r5YSQs46iejeB96q0MAy9Ko7hkgo/'\n",
    "destination_directory = '../data/'\n",
    "\n",
    "for file in files:\n",
    "    url = base_url + file\n",
    "    destination = destination_directory + file\n",
    "    print(\"Downloading '{}'' to location '{}'\".format(url, destination))\n",
    "    urlretrieve(url, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Dataset\n",
    "\n",
    "TODO: Import dataset.py and explain how it's used + what it does.\n",
    "\n",
    "TODO: Add Sam's scrub function in here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_dataset = pandas.read_csv(\"../data/dataset.csv.gz\", header=None, names=['repo', 'language', 'documents'])\n",
    "minimal_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pandas.read_csv(\"../data/full-dataset.csv.gz\", header=None, names=['repo', 'language',  'topics', 'documents'])\n",
    "\n",
    "# Remove Github 'topics' since we don't use them in this analysis\n",
    "full_dataset = full_dataset.drop(columns='topics')\n",
    "\n",
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pandas.read_csv(\"../data/test-dataset.csv.gz\", header=None, names=['repo', 'language', 'topics', 'documents'])\n",
    "\n",
    "# Remove Github 'topics' since we don't use them in this analysis\n",
    "test_dataset = test_dataset.drop(columns='topics')\n",
    "\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the mixture model, we must label each repository with it's percentage of each programming language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_language_percentages(group):\n",
    "    total_python_length = 0\n",
    "    total_r_length = 0\n",
    "    total_javascript_length = 0\n",
    "    \n",
    "    for index, repo, language, document in group.itertuples():\n",
    "        if language == 'python':\n",
    "            total_python_length += len(document)\n",
    "            \n",
    "        if language == 'javascript':\n",
    "            total_javascript_length += len(document)\n",
    "            \n",
    "        if language == 'r':\n",
    "            total_r_length += len(document)\n",
    "            \n",
    "    total_length = total_python_length + total_r_length + total_javascript_length\n",
    "            \n",
    "    return pandas.Series([\n",
    "        total_python_length/total_length,\n",
    "        total_r_length/total_length,\n",
    "        total_javascript_length/total_length,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composition_actual = test_dataset.groupby(by='repo').apply(calculate_language_percentages)\n",
    "test_composition_actual.columns = ['python', 'r', 'javascript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the programming language percentages for each of repository in our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_composition_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_texts(group):\n",
    "    [repo_id] = group['repo'].unique()\n",
    "    combined = ' '.join(group['documents'])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_documents = test_dataset.groupby(by='repo').apply(concat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling on Individual Source Files\n",
    "\n",
    "Basically done, just need to copy it over. Maybe run on the bigger dataset?\n",
    "\n",
    "TODO:\n",
    "  - Copy work from documentation/daniel-jones.ipynb\n",
    "  - Add visualisation with pyldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, common words are important and rare words aren't. So we shouldn't use tf-idf as a metric, bag-of-words makes more sense. (TODO: Maybe: \"Similarly, filter out words that don't occur very often\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = minimal_dataset['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(stop_words=None)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "with open('../data/minimal_lda_tf.pickle', 'wb') as f:\n",
    "    pickle.dump(tf, f)\n",
    "    \n",
    "with open('../data/minimal_lda_tf_vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four programming languages, try to use LDA to determine these four programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_languages = 4\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=number_of_languages,  n_jobs=1)\n",
    "model = lda.fit(tf)\n",
    "\n",
    "with open('../data/minimal_lda_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/minimal_lda_model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open('../data/minimal_lda_tf.pickle', 'rb') as f:\n",
    "    tf = pickle.load(f)\n",
    "    \n",
    "with open('../data/minimal_lda_tf_vectorizer.pickle', 'rb') as f:\n",
    "    tf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(model, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to do this on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_languages = 4\n",
    "all_documents = full_dataset['documents']\n",
    "\n",
    "full_tf_vectorizer = CountVectorizer(stop_words=None)\n",
    "full_tf = full_tf_vectorizer.fit_transform(all_documents)\n",
    "full_tf_feature_names = full_tf_vectorizer.get_feature_names()\n",
    "\n",
    "full_lda = LatentDirichletAllocation(n_topics=number_of_languages,  n_jobs=1)\n",
    "full_model = full_lda.fit(full_tf)\n",
    "\n",
    "with open('../data/full_lda_model.pickle', 'wb') as f:\n",
    "    pickle.dump(full_model, f)\n",
    "\n",
    "with open('../data/full_tf.pickle', 'wb') as f:\n",
    "    pickle.dump(full_tf, f)\n",
    "    \n",
    "with open('../data/full_tf_vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(full_tf_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/full_lda_model.pickle', 'rb') as f:\n",
    "    full_model = pickle.load(f)\n",
    "\n",
    "with open('../data/full_tf.pickle', 'rb') as f:\n",
    "    full_tf = pickle.load(f)\n",
    "    \n",
    "with open('../data/full_tf_vectorizer.pickle', 'rb') as f:\n",
    "    full_tf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize our new model. Note that the following code cell requires more than 8 GB of RAM to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(full_model, full_tf, full_tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling on Programming Language Mixtures\n",
    "Keypoint: topics are programming languages, file with mixture of programming languages, identify which is which.\n",
    "\n",
    "Applicability to cyber-security: identifying malware embedded within normal programs (shellcode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Source Files per Repository\n",
    "\n",
    "Currently our data set consists of one data point per file containing:\n",
    "  1. ID of the Github repository the file belongs to.\n",
    "  2. Programming language it is written in (identified by file extension).\n",
    "  3. File contents\n",
    "  \n",
    "In this section, we extend our analysis from working on documents with one language per file, to a system where there is a mixture of languages inside each document. To do this, we combine all the files in each repository into a single data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_documents = minimal_dataset.groupby(by='repo').apply(concat_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_documents.append(combined_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(stop_words=None)\n",
    "tf_vectorizer.fit(combined_test_documents.append(combined_documents))\n",
    "\n",
    "tf_train = tf_vectorizer.transform(combined_documents)\n",
    "tf_test = tf_vectorizer.transform(combined_test_documents)\n",
    "\n",
    "with open('../data/concatDocs_tf_vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_vectorizer, f)\n",
    "    \n",
    "with open('../data/concatDocs_tf_train.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_train, f)\n",
    "    \n",
    "with open('../data/concatDocs_tf_test.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_languages = 4\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=number_of_languages,  n_jobs=1)\n",
    "model = lda.fit(tf_train)\n",
    "\n",
    "with open('../data/concatDocs_lda_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/concatDocs_lda_model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open('../data/concatDocs_tf_train.pickle', 'rb') as f:\n",
    "    tf_train = pickle.load(f)\n",
    "    \n",
    "with open('../data/concatDocs_tf_vectorizer.pickle', 'rb') as f:\n",
    "    tf_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('../data/concatDocs_tf_test.pickle', 'rb') as f:\n",
    "    tf_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(model, tf_train, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = dict(enumerate(tf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(model.components_), len(model.components_[0]), len(feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_of_feature_in_topic = model.components_[<topic_index>][<feature_index>]\n",
    "\n",
    "word_importance_per_topic = []\n",
    "\n",
    "for topic_components in model.components_:\n",
    "    word_importance = [\n",
    "        (feature_map[feature_index], feature_importance) \n",
    "        for feature_index, feature_importance in enumerate(topic_components)\n",
    "    ]\n",
    "    word_importance = sorted(word_importance, key=lambda tup: tup[1], reverse=True)\n",
    "    word_importance_per_topic.append(word_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: visualise via heatmap?\n",
    "\n",
    "most_importance_topic_1_keywords = [word for word, importance in word_importance_per_topic[0][0:40]]\n",
    "most_importance_topic_2_keywords = [word for word, importance in word_importance_per_topic[1][0:40]]\n",
    "most_importance_topic_3_keywords = [word for word, importance in word_importance_per_topic[2][0:40]]\n",
    "most_importance_topic_4_keywords = [word for word, importance in word_importance_per_topic[3][0:40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    100*jaccard_index(javascript_keywords, most_importance_topic_1_keywords)/len(javascript_keywords),\n",
    "    100*jaccard_index(javascript_keywords, most_importance_topic_2_keywords)/len(javascript_keywords),\n",
    "    100*jaccard_index(javascript_keywords, most_importance_topic_3_keywords)/len(javascript_keywords),\n",
    "    100*jaccard_index(javascript_keywords, most_importance_topic_4_keywords)/len(javascript_keywords),\n",
    ")\n",
    "\n",
    "print(\n",
    "    100*jaccard_index(r_keywords, most_importance_topic_1_keywords)/len(r_keywords),\n",
    "    100*jaccard_index(r_keywords, most_importance_topic_2_keywords)/len(r_keywords),\n",
    "    100*jaccard_index(r_keywords, most_importance_topic_3_keywords)/len(r_keywords),\n",
    "    100*jaccard_index(r_keywords, most_importance_topic_4_keywords)/len(r_keywords),\n",
    ")\n",
    "\n",
    "print(\n",
    "    100*jaccard_index(python_keywords, most_importance_topic_1_keywords)/len(python_keywords),\n",
    "    100*jaccard_index(python_keywords, most_importance_topic_2_keywords)/len(python_keywords),\n",
    "    100*jaccard_index(python_keywords, most_importance_topic_3_keywords)/len(python_keywords),\n",
    "    100*jaccard_index(python_keywords, most_importance_topic_4_keywords)/len(python_keywords),\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Splitting Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential problem we identified was with the method that sklearn uses to split up the program into \"words\". The base function searches the program for strings of two or more alphanumeric characters in a row, taking these as the words of a particular document. The issue here is that most code contains comments (which are heavy in alphanumeric characters that offer little to no help in detecting the programming language (or at least it is hard to detect the phrases that are helpful)), and also non-alphanumeric characters that might help to detect the language (for instance certain forms of brackets, colons, and other indicators). Therefore we set to design a new splitting function which would hopefully filter out non-useful information and leave us with a list of words that has more to say about the language being written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(text):\n",
    "    if type(text)==str:\n",
    "        return text.lstrip().rstrip()\n",
    "    elif type(text)==list:\n",
    "        tram=[]\n",
    "        for i in text:\n",
    "            tram.append(i.lstrip().rstrip())\n",
    "        return tram\n",
    "    else:\n",
    "        return \"Not Correct Format\"\n",
    "def splat(text, splt):\n",
    "    if type(text)==str:\n",
    "        splot=str.split(text[2:-1], splt)\n",
    "        return trim(splot)\n",
    "    elif type(text)==list:\n",
    "        splot=[]\n",
    "        for i in text:\n",
    "            for j in str.split(i,splt):\n",
    "                splot.append(j)\n",
    "        return trim(splot)\n",
    "    else:\n",
    "        return \"Not Correct Format\"\n",
    "def scrub(string, inclComments=False):\n",
    "    x=string\n",
    "    x=splat(x,\"\\\\n\")\n",
    "    if inclComments==False:\n",
    "        x=[i for i in x if i[0:2]!=\"//\"]\n",
    "        x=[i for i in x if i[0:1]!=\"#\"]\n",
    "    x=splat(x,\"(\")\n",
    "    x=splat(x,\"[\")\n",
    "    x=splat(x,\"{\")\n",
    "    x=splat(x,\" \")\n",
    "    x=[i for i in x if i!=\"\"]\n",
    "    return x\n",
    "def concatMe(a):\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply an almost identical method as before to run LDA on these new modified documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_split_documents=combined_documents.apply(scrub).apply(concatMe)\n",
    "combined_split_test_documents=combined_test_documents.apply(scrub).apply(concatMe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tf_vectorizer = CountVectorizer(stop_words=None, token_pattern=\"[^ ]*\")\n",
    "split_tf_vectorizer.fit(combined_split_test_documents.append(combined_split_documents))\n",
    "\n",
    "split_tf_train = split_tf_vectorizer.transform(combined_split_documents)\n",
    "split_tf_test = split_tf_vectorizer.transform(combined_split_test_documents)\n",
    "\n",
    "with open('../data/concatDocs_split_tf_vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(split_tf_vectorizer, f)\n",
    "    \n",
    "with open('../data/concatDocs_split_tf_train.pickle', 'wb') as f:\n",
    "    pickle.dump(split_tf_train, f)\n",
    "    \n",
    "with open('../data/concatDocs_split_tf_test.pickle', 'wb') as f:\n",
    "    pickle.dump(split_tf_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_languages = 4\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=number_of_languages,  n_jobs=1)\n",
    "model = lda.fit(split_tf_train)\n",
    "\n",
    "with open('../data/concatDocs_split_lda_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/concatDocs_split_lda_model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open('../data/concatDocs_split_tf_train.pickle', 'rb') as f:\n",
    "    tf_train = pickle.load(f)\n",
    "    \n",
    "with open('../data/concatDocs_split_tf_vectorizer.pickle', 'rb') as f:\n",
    "    tf_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('../data/concatDocs_split_tf_test.pickle', 'rb') as f:\n",
    "    tf_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(model, tf_train, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Program Subjects and Themes\n",
    "\n",
    "Hypothesis: Using tf-idf rather than bag-of-words as an input to LDA will prioritise rare words. In the case of source code, this means programming language keywords (an identifying feature of programming languages) are deprioritised, and so a more human idea of topics may emerge. \n",
    "\n",
    "We can use repo-list.json and the repo-ids to map the github topics/tags to each repo. Might be a small/easy task to compare against the programming langauge identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = minimal_dataset['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words=None)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "with open('../data/tfidf_lda_tf.pickle', 'wb') as f:\n",
    "    pickle.dump(tf, f)\n",
    "    \n",
    "with open('../data/tfidf_lda_tf_vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four programming languages, try to use LDA to determine these four programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_themes = 3\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=number_of_themes,  n_jobs=1)\n",
    "model = lda.fit(tf)\n",
    "\n",
    "with open('../data/tfidf_lda_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tfidf_lda_model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open('../data/tfidf_lda_tf.pickle', 'rb') as f:\n",
    "    tf = pickle.load(f)\n",
    "    \n",
    "with open('../data/tfidf_lda_tf_vectorizer.pickle', 'rb') as f:\n",
    "    tf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(model, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still prioritises programming language keywords. One approach to solving this problem is to consider all keywords as \"stopwords\". First, gather a list of R, Python and Javascript keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyword\n",
    "\n",
    "python_keywords = keyword.kwlist\n",
    "python_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R reserved words (sourced from the manual: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Reserved.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_keywords = [\n",
    "    \"if\", \n",
    "    \"else\", \n",
    "    \"repeat\",\n",
    "    \"while\",\n",
    "    \"function\", \n",
    "    \"for\",\n",
    "    \"in\",\n",
    "    \"next\",\n",
    "    \"break\",\n",
    "    \"TRUE\",\n",
    "    \"FALSE\",\n",
    "    \"NULL\", \n",
    "    \"Inf\", \n",
    "    \"NaN\",\n",
    "    \"NA\",\n",
    "    \"NA_integer_\",\n",
    "    \"NA_real_\",\n",
    "    \"NA_complex_\",\n",
    "    \"NA_character_\", \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Javascript keywords and reserved words (source: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Lexical_grammar#Keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javascript_keywords = [  # jaccard(\"ideal javascript topic\", topic_i)\n",
    "    \"break\",\n",
    "    \"case\",\n",
    "    \"catch\",\n",
    "    \"class\",\n",
    "    \"const\",\n",
    "    \"continue\",\n",
    "    \"debugger\",\n",
    "    \"default\",\n",
    "    \"delete\",\n",
    "    \"do\",\n",
    "    \"else\",\n",
    "    \"export\",\n",
    "    \"extends\",\n",
    "    \"finally\",\n",
    "    \"for\",\n",
    "    \"function\",\n",
    "    \"if\",\n",
    "    \"import\",\n",
    "    \"in\",\n",
    "    \"instanceof\",\n",
    "    \"new\",\n",
    "    \"return\",\n",
    "    \"super\",\n",
    "    \"switch\",\n",
    "    \"this\",\n",
    "    \"throw\",\n",
    "    \"try\",\n",
    "    \"typeof\",\n",
    "    \"var\",\n",
    "    \"void\",\n",
    "    \"while\",\n",
    "    \"with\",\n",
    "    \"yield\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = minimal_dataset['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words=javascript_keywords+python_keywords+r_keywords)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "with open('../data/tfidf_lda_tf_ignore_keywords.pickle', 'wb') as f:\n",
    "    pickle.dump(tf, f)\n",
    "    \n",
    "with open('../data/tfidf_lda_tf_vectorizer_ignore_keywords.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four programming languages, try to use LDA to determine these four programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_themes = 3\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=number_of_themes,  n_jobs=1)\n",
    "model = lda.fit(tf)\n",
    "\n",
    "with open('../data/tfidf_lda_model_ignore_keywords.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tfidf_lda_model_ignore_keywords.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open('../data/tfidf_lda_tf_ignore_keywords.pickle', 'rb') as f:\n",
    "    tf = pickle.load(f)\n",
    "    \n",
    "with open('../data/tfidf_lda_tf_vectorizer_ignore_keywords.pickle', 'rb') as f:\n",
    "    tf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(model, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Efficacy of Topic Models (WRITEUP: WHO?)\n",
    "\n",
    "Main question: How do we evaluate how well a topic (from LDA for example) represents a meaningful \"topic\" or theme?\n",
    "\n",
    "TODO: do some research on this??? There must be some papers etc that try to formalise this that we can borrow ideas from?\n",
    "\n",
    "Paper dump:\n",
    "  - Looks like a good summary paper: http://www.aclweb.org/anthology/E14-4005 Find more papers from this ones references?\n",
    "    - \" KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). \"\n",
    "    - \"Kim and Oh (2011) also applied  the  cosine  measure  and  KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount  Cumulative  Gain  and  Jensen  Shannon  Divergence (JSD).\"\n",
    "  - Cool name haven't read it: http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
    "  \n",
    "We considered all of these metrics, and found th Jaccard Index to be most suitable. This was primarily due to it's use of set operations, which are invariant to ordering and number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Tea Leaves Paper\n",
    "http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Overlap\n",
    "\n",
    "I think this is used as a baseline measure in the summary paper above (http://www.aclweb.org/anthology/E14-4005). Should be a quick implementation so worth a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Index\n",
    "\n",
    "From the papers above this seems to have been used relatively often for _linking machine-generated topics to human topics_ and so maybe this is a good application for it. Apparently explored here \"https://link.springer.com/chapter/10.1007/978-3-642-19437-5_13\" but I haven't read it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    \n",
    "    num_shared = float(len(a & b))\n",
    "    num_total = float(len(a | b))\n",
    "    \n",
    "    return num_shared/num_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the group of language keywords as the best possible topic for each language. Compare each of our machine generated topics with each of our ideal topics by computing their Jaccard Index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the group of language keywords as the best possible topic for each language. Compare each of our machine generated topics with each of our ideal topics by computing their Jaccard Index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1_keywords = [\n",
    "\n",
    "\"this\",\n",
    "\"function\",\n",
    "\"if\",\n",
    "\"the\",\n",
    "\"return\",\n",
    "\"var\",\n",
    "\"to\",\n",
    "\"for\",\n",
    "\"is\",\n",
    "\"true\",\n",
    "\"in\",\n",
    "\"of\",\n",
    "\"value\",\n",
    "\"data\",\n",
    "\"null\",\n",
    "\"length\",\n",
    "\"and\",\n",
    "\"else\",\n",
    "\"false\",\n",
    "\"name\",\n",
    "\"type\",\n",
    "\"new\",\n",
    "\"const\",\n",
    "\"it\",\n",
    "\"assert\",\n",
    "\"object\",\n",
    "\"be\",\n",
    "\"options\",\n",
    "\"key\",\n",
    "\"that\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic3_keywords = [\n",
    "\"self\",\n",
    "\"def\",\n",
    "\"import\",\n",
    "\"from\",\n",
    "\"none\",\n",
    "\"user\",\n",
    "\"in\",\n",
    "\"ndef\",\n",
    "\"response\",\n",
    "\"name\",\n",
    "\"assert\",\n",
    "\"equal\",\n",
    "\"id\",\n",
    "\"not\",\n",
    "\"str\",\n",
    "\"models\",\n",
    "\"true\",\n",
    "\"request\",\n",
    "\"get\",\n",
    "\"email\",\n",
    "\"assert\",\n",
    "\"realm\",\n",
    "\"data\",\n",
    "\"message\",\n",
    "\"result\",\n",
    "\"for\",\n",
    "\"nclass\",\n",
    "\"dict\",\n",
    "\"django\",\n",
    "\"thread\",\n",
    "\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4_keywords = [\n",
    "\"react\",\n",
    "\"from\",\n",
    "\"default\",\n",
    "\"nimport\",\n",
    "\"nexport\",\n",
    "\"createsvgicon\",\n",
    "\"path\",\n",
    "\"import\",\n",
    "\"fragment\",\n",
    "\"xd0\",\n",
    "\"xe0\",\n",
    "\"classname\",\n",
    "\"as\",\n",
    "\"props\",\n",
    "\"utils\",\n",
    "\"none\",\n",
    "\"div\",\n",
    "\"proptypes\",\n",
    "\"createelement\",\n",
    "\"theme\",\n",
    "\"xe1\",\n",
    "\"material\",\n",
    "\"xd1\",\n",
    "\"button\",\n",
    "\"classes\",\n",
    "\"m0\",\n",
    "\"fill\",\n",
    "\"ui\",\n",
    "\"xe2\",\n",
    "\"0h24v24h0v0z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jaccard_index(javascript_keywords, topic1_keywords),\n",
    "jaccard_index(javascript_keywords, topic3_keywords),\n",
    "jaccard_index(javascript_keywords, topic4_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jaccard_index(r_keywords, topic1_keywords),\n",
    "jaccard_index(r_keywords, topic3_keywords),\n",
    "jaccard_index(r_keywords, topic4_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jaccard_index(python_keywords, topic1_keywords),\n",
    "jaccard_index(python_keywords, topic3_keywords),\n",
    "jaccard_index(python_keywords, topic4_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kendall’s τ Coefficient\n",
    "\n",
    "Measures the association between two ranked lists. Source: Computational Linguistics and Intelligent Text Processing book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Topic Models\n",
    "TODO: better title needed\n",
    "\n",
    "Idea:\n",
    "  - save the actual % of each program langauge per repo\n",
    "  - Then try to use LDA model to tell us \"I believe repo <x> is 10% Topic 1, 20% Topic 2 etc\". \n",
    "  - Use analysis from above two sections to create a \"most likely mapping from lda topic to programming language\".\n",
    "  - rate our models\n",
    "\n",
    "Here we can do cross-validation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pandas.read_csv(\"../data/test-dataset.csv.gz\", header=None, names=['repo', 'language', 'topics', 'documents'])\n",
    "\n",
    "# Remove Github 'topics' since we don't use them in this analysis\n",
    "test_dataset = test_dataset.drop(columns='topics')\n",
    "\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the mixture model, we must label each repository with it's percentage of each programming language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_language_percentages(group):\n",
    "    total_python_length = 0\n",
    "    total_r_length = 0\n",
    "    total_javascript_length = 0\n",
    "    \n",
    "    for index, repo, language, document in group.itertuples():\n",
    "        if language == 'python':\n",
    "            total_python_length += len(document)\n",
    "            \n",
    "        if language == 'javascript':\n",
    "            total_javascript_length += len(document)\n",
    "            \n",
    "        if language == 'r':\n",
    "            total_r_length += len(document)\n",
    "            \n",
    "    total_length = total_python_length + total_r_length + total_javascript_length\n",
    "            \n",
    "    return pandas.Series([\n",
    "        total_python_length/total_length,\n",
    "        total_r_length/total_length,\n",
    "        total_javascript_length/total_length,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composition_actual = test_dataset.groupby(by='repo').apply(calculate_language_percentages)\n",
    "test_composition_actual.columns = ['python', 'r', 'javascript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the programming language percentages for each of repository in our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_composition_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_documents = test_dataset.groupby(by='repo').apply(concat_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/concatDocs_lda_model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open('../data/concatDocs_tf_train.pickle', 'rb') as f:\n",
    "    tf_train = pickle.load(f)\n",
    "    \n",
    "with open('../data/concatDocs_tf_vectorizer.pickle', 'rb') as f:\n",
    "    tf_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('../data/concatDocs_tf_test.pickle', 'rb') as f:\n",
    "    tf_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_documents_tf = tf_vectorizer.transform(combined_test_documents)\n",
    "combined_test_model = model.transform(combined_test_documents_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert kish's mappings here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composition_estim=test_composition_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(test_composition_estim.index==test_composition_actual.index)==True:\n",
    "    KDDataframe=pandas.DataFrame([],columns=[\"Repo\",\"KD Divergence\"])\n",
    "    for i in range(0,len(test_composition_actual)):\n",
    "        a=0\n",
    "        for j in range(0,len(test_composition_actual.iloc[i])):\n",
    "            a=a-((test_composition_actual.iloc[i][j])*math.log(test_composition_actual.iloc[i][j]/test_composition_estim.iloc[i][j]))\n",
    "        #m=pandas.DataFrame([test_composition_actual.index[i],a],columns=[\"Repo\",\"KD Divergence\"])\n",
    "        print(a)\n",
    "        #KDDataframe.append(m)\n",
    "#KDDataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "  - our keyword lists have simliar/common words e.g. python javascript and r all share some keywords. this can be seen in the documents. choosing another language, with a completely different set of keywords might prove easier to differentiate for the LDA model. somethind somthing LDA uses distance, but if true topics share key words, then distance metric breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
