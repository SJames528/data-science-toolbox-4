{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on Program Source Code\n",
    "---\n",
    "By Kishalay Banerjee, Dan Jones and Sam Harding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(0xD00D5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Dataset\n",
    "\n",
    "TODO: Import dataset.py and explain how it's used + what it does.\n",
    "\n",
    "TODO: Add Sam's scrub function in here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_dataset = pandas.read_csv(\"../data/dataset.csv.gz\", header=None, names=['repo', 'language', 'documents'])\n",
    "minimal_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling on Individual Source Files\n",
    "\n",
    "Basically done, just need to copy it over. Maybe run on the bigger dataset?\n",
    "\n",
    "TODO:\n",
    "  - Copy work from documentation/daniel-jones.ipynb\n",
    "  - Add visualisation with pyldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, common words are important and rare words aren't. So we shouldn't use tf-idf as a metric, bag-of-words makes more sense. (TODO: Maybe: \"Similarly, filter out words that don't occur very often\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(stop_words=None)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four programming languages, try to use LDA to determine these four programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_languages = 4\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=number_of_languages,  n_jobs=1)\n",
    "model = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/minimal_lda_model.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/minimal_lda_model.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(model, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling on Programming Language Mixtures\n",
    "Keypoint: topics are programming languages, file with mixture of programming languages, identify which is which.\n",
    "\n",
    "Applicability to cyber-security: identifying malware embedded within normal programs (shellcode).\n",
    "\n",
    "### Combining Source Files per Repository\n",
    "\n",
    "TODO:\n",
    "  - Copy work Sam's\n",
    "  \n",
    "  \n",
    "### LDA Model\n",
    "\n",
    "TODO:\n",
    "  - Write model\n",
    "  - Add visualisation with pyldavis\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying more general topics/themes in program source code (DAN)\n",
    "\n",
    "Hypothesis: Using tf-idf rather than bag-of-words as an input to LDA will prioritise rare words. In the case of source code, this means programming language keywords (an identifying feature of programming languages) are deprioritised, and so a more human idea of topics may emerge. \n",
    "\n",
    "We can use repo-list.json and the repo-ids to map the github topics/tags to each repo. Might be a small/easy task to compare against the programming langauge identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Efficacy of Topic Models (WRITEUP: WHO?)\n",
    "\n",
    "Main question: How do we evaluate how well a topic (from LDA for example) represents a meaningful \"topic\" or theme?\n",
    "\n",
    "TODO: do some research on this??? There must be some papers etc that try to formalise this that we can borrow ideas from?\n",
    "\n",
    "Paper dump:\n",
    "  - Looks like a good summary paper: http://www.aclweb.org/anthology/E14-4005 Find more papers from this ones references?\n",
    "    - \" KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). \"\n",
    "    - \"Kim and Oh (2011) also applied  the  cosine  measure  and  KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount  Cumulative  Gain  and  Jensen  Shannon  Divergence (JSD).\"\n",
    "  - Cool name haven't read it: http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Tea Leaves Paper\n",
    "http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Overlap\n",
    "\n",
    "I think this is used as a baseline measure in the summary paper above (http://www.aclweb.org/anthology/E14-4005). Should be a quick implementation so worth a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Index\n",
    "\n",
    "From the papers above this seems to have been used relatively often for _linking machine-generated topics to human topics_ and so maybe this is a good application for it. Apparently explored here \"https://link.springer.com/chapter/10.1007/978-3-642-19437-5_13\" but I haven't read it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing topic mixtures generated by model with measured mixtures, i.e. how good are our models?\n",
    "TODO: better title needed\n",
    "\n",
    "Idea:\n",
    "  - save the actual % of each program langauge per repo\n",
    "  - Then try to use LDA model to tell us \"I believe repo <x> is 10% Topic 1, 20% Topic 2 etc\". \n",
    "  - Use analysis from above two sections to create a \"most likely mapping from lda topic to programming language\".\n",
    "  - rate our models\n",
    "\n",
    "Here we can do cross-validation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
